{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluación de Modelos\n",
    "\n",
    "Ahora que nos hemos familiarizado con nuestros datos, el próximo paso lógico es explorar el espacio de los algoritmos que eventualmente producirán un buen modelo para la tarea que buscamos resolver.\n",
    "\n",
    "Nuestra meta en este notebook no es desarrollar una solución vanguardista, sino, más bien, revisar diversas arquitecturas con el fin de ver cuáles serán promovidas a la siguiente etapa del proceso, centrada en la optimización.\n",
    "\n",
    "Sin más preámbulos, pongámonos manos a la obra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "En computer vision siempre es buena idea empezar apoyándonos en el conocimiento de modelos pre-entrenados. Esta técnica se conoce como _transfer learning_.\n",
    "\n",
    "Keras ya viene con una serie de modelos entrenados en ImageNet, lo cual es genial. Esta vez utilizaremos la inferfaz de alto nivel de Keras que se halla dentro de TensorFlow, en vez de su versión independiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión\n",
    "\n",
    "Para evaluar un amplio espectro de posibles algoritmos, necesitamos primero implementar algunos métodos. Empecemos por darle forma a la data que usaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos\n",
    "\n",
    "Dado que estamos revisando redes neuronales profundas, debemos preservar tanta memoria como sea posible. Es por este motivo que generaremos lotes de datos bajo demanda, directamente desde el disco, utilizando `flow_from_directory`.\n",
    "\n",
    "Esta función espera que las imágenes correspondientes a una clase se encuentren dentro de un subdirectorio con el nombre de la misma. Es por eso que debemos reajustar nuestra estructura de directorios. \n",
    "\n",
    "También apartaremos el 10% de nuestro conjunto de datos para validar que los modelos estén aprendiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "destination_directory = './dataset'\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return image\n",
    "\n",
    "VALIDATION_PROPORTION = 0.1\n",
    "vehicles_images_path = shuffle(glob.glob('data/vehicles/*/*.png'))\n",
    "split_point = int(len(vehicles_images_path) * VALIDATION_PROPORTION)\n",
    "\n",
    "for i, image_path in enumerate(vehicles_images_path):\n",
    "    image = load_image(image_path)\n",
    "    \n",
    "    if i < split_point:\n",
    "        destination_path = os.path.join(destination_directory, 'valid', 'vehicle', f'{i}.png')\n",
    "    else:\n",
    "        destination_path = os.path.join(destination_directory, 'train', 'vehicle', f'{i}.png')\n",
    "    \n",
    "    cv2.imwrite(destination_path, image)\n",
    "    \n",
    "non_vehicles_images_path = shuffle(glob.glob('data/non-vehicles/*/*.png'))\n",
    "split_point = int(len(non_vehicles_images_path) * VALIDATION_PROPORTION)\n",
    "for i, image_path in enumerate(non_vehicles_images_path):\n",
    "    image = load_image(image_path)\n",
    "    \n",
    "    if i < split_point:\n",
    "        destination_path = os.path.join(destination_directory, 'valid', 'non_vehicle', f'{i}.png')\n",
    "    else:\n",
    "        destination_path = os.path.join(destination_directory, 'train', 'non_vehicle', f'{i}.png')\n",
    "    \n",
    "    cv2.imwrite(destination_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya mencionamos anteriormente, el aspecto positivo del método `flow_from_directory` es que crea mapeos para las etiquetas con base en la estructura de subdirectorios donde yacen los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos\n",
    "\n",
    "Ahora, procederemos a definir una función para obtener los modelos que deseamos evaluar.\n",
    "\n",
    "La función `get_models` retornará un `dict` de `dict`s, donde las claves del diccionario externo son los nombres del modelo pre-entrenado que estamos usando, y los valores son diccionarios que contienen la función de preprocesamiento asociada al modelo pre-entrenado, una función para construir el modelo propiamente y las dimensiones de entrada que éste espera.\n",
    "\n",
    "La sub-función `get_model_with_new_top` toma una _base_ (modelo pre-entrenado) y le coloca una red neuronal totalmente conectada (también conocidas como perceptrones multicapa) encima. También congela todas las capas en la base. Por último, compila el modelo para que utilice el optimizador `adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "\n",
    "SEED = 314159\n",
    "\n",
    "def get_models(models=None):\n",
    "    # Takes a base, pretrained model, and attaches a new FCN on top of it.\n",
    "    def get_model_with_new_top(base_model):\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        predictions = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        \n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    if models is None:\n",
    "        models = dict()\n",
    "        \n",
    "    models['mobilenet'] = {\n",
    "        'preprocessing_function': applications.mobilenet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['resnet50'] = {\n",
    "        'preprocessing_function': applications.resnet50.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['inceptionV3'] = {\n",
    "        'preprocessing_function': applications.inception_v3.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['xception'] = {\n",
    "        'preprocessing_function': applications.xception.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))), \n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['nasnet_large'] = {\n",
    "        'preprocessing_function': applications.nasnet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.NASNetLarge(weights='imagenet', include_top=False, input_shape=(331, 331, 3))),\n",
    "        'input_shape': (331, 331)\n",
    "    }\n",
    "    \n",
    "    models['nasnet_mobile'] = {\n",
    "        'preprocessing_function': applications.nasnet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['densenet121'] = {\n",
    "        'preprocessing_function': applications.densenet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['densenet169'] = {\n",
    "        'preprocessing_function': applications.densenet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.DenseNet169(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['densenet201'] = {\n",
    "        'preprocessing_function': applications.densenet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['inception_resnet_v2'] = {\n",
    "        'preprocessing_function': applications.inception_resnet_v2.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(299, 299, 3))),\n",
    "        'input_shape': (299, 299)\n",
    "    }\n",
    "    \n",
    "    models['vgg16'] = {\n",
    "        'preprocessing_function': applications.vgg16.preprocess_input, \n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))), \n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['vgg19'] = {\n",
    "        'preprocessing_function': applications.vgg19.preprocess_input, \n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))), \n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos notar en la celda anterior, estamos utilizando un amplio rango de _bases_, desde modelos pequeños hasta unos bastante grandes.\n",
    "\n",
    "Puesto que nuestros datos son lo suficientemente pequeños y guardan cierta relación con ImageNet, sólo entrenaremos las capas del perceptrón multicapa que se encuentra encima del modelo pre-entrenado. De manera más concreta, el mayor beneficio que estamos cosechando al usar _transfer learning_ es la extracción de _features_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación\n",
    "\n",
    "Es hora de evaluar el desempeño de cada candidato. Para ello, crearemos una función llamada `train_and_evaluate_models`, la cual recibe un `dict` de modelos (tal y como los retorna `get_models`), un número de _epochs_, y entrena cada clasificador por ese número de iteraciones.\n",
    "\n",
    "Para medir el rendimiento, usaremos la data de validación que apartamos hace un par de celdas. Esto nos permitirá conocer qué tan bien le va a cada modelo sobre imágenes nunca vistas.\n",
    "\n",
    "También estamos iteresados en mantener nuestra solución lo más magra posible, por lo que conocer el número de parametros es relevante.\n",
    "\n",
    "Finalmente, es destacable el hecho de que estamos limpiando la sesión y desechando cada modelo una vez hemos terminado con él. Esto es fundamental dado que, de lo contrario, nuestra computadora se rompería por falta de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(models, epochs=5):\n",
    "    for model_name, model_data in models.items():\n",
    "        m = model_data['model_constructor']()\n",
    "        train_data_generator = ImageDataGenerator(preprocessing_function=model_data['preprocessing_function']).flow_from_directory('./dataset/train', \n",
    "                                                                                                                                   target_size=model_data['input_shape'],\n",
    "                                                                                                                                   batch_size=32,\n",
    "                                                                                                                                   class_mode='binary')\n",
    "        \n",
    "        valid_data_generator = ImageDataGenerator(preprocessing_function=model_data['preprocessing_function']).flow_from_directory('./dataset/valid', \n",
    "                                                                                                                                   target_size=model_data['input_shape'],\n",
    "                                                                                                                                   batch_size=32,\n",
    "                                                                                                                                   class_mode='binary')\n",
    "        step_size_train = train_data_generator.n // train_data_generator.batch_size\n",
    "\n",
    "        print(f'Training {model_name}')\n",
    "        history = m.fit_generator(generator=train_data_generator,\n",
    "                                  steps_per_epoch=step_size_train,\n",
    "                                  validation_data=valid_data_generator,\n",
    "                                  validation_steps=(valid_data_generator.n // valid_data_generator.batch_size),\n",
    "                                  epochs=epochs)\n",
    "\n",
    "        print('Number of parameters: {:,}'.format(m.count_params()))\n",
    "        print('---------------\\n\\n')\n",
    "\n",
    "        del m\n",
    "        del history\n",
    "        K.clear_session()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Estamos listos! Hora de poner a prueba a los candidatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training mobilenet\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 3s 136ms/step - loss: 0.1482 - acc: 0.9467\n",
      "207/207 [==============================] - 27s 132ms/step - loss: 0.1239 - acc: 0.9540 - val_loss: 0.1482 - val_acc: 0.9467\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 2s 102ms/step - loss: 0.0569 - acc: 0.9822\n",
      "207/207 [==============================] - 25s 120ms/step - loss: 0.0554 - acc: 0.9791 - val_loss: 0.0569 - val_acc: 0.9822\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 2s 102ms/step - loss: 0.1439 - acc: 0.9399\n",
      "207/207 [==============================] - 24s 118ms/step - loss: 0.0510 - acc: 0.9800 - val_loss: 0.1439 - val_acc: 0.9399\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 2s 102ms/step - loss: 0.0527 - acc: 0.9809\n",
      "207/207 [==============================] - 24s 117ms/step - loss: 0.0396 - acc: 0.9848 - val_loss: 0.0527 - val_acc: 0.9809\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 2s 102ms/step - loss: 0.0580 - acc: 0.9822\n",
      "207/207 [==============================] - 24s 117ms/step - loss: 0.0564 - acc: 0.9801 - val_loss: 0.0580 - val_acc: 0.9822\n",
      "Number of parameters: 3,885,249\n",
      "---------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training resnet50\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 8s 357ms/step - loss: 0.0534 - acc: 0.9809\n",
      "207/207 [==============================] - 74s 356ms/step - loss: 0.0916 - acc: 0.9630 - val_loss: 0.0534 - val_acc: 0.9809\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 7s 291ms/step - loss: 0.0938 - acc: 0.9672\n",
      "207/207 [==============================] - 70s 338ms/step - loss: 0.0523 - acc: 0.9809 - val_loss: 0.0938 - val_acc: 0.9672\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 7s 288ms/step - loss: 0.0544 - acc: 0.9768\n",
      "207/207 [==============================] - 70s 337ms/step - loss: 0.0279 - acc: 0.9909 - val_loss: 0.0544 - val_acc: 0.9768\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 7s 294ms/step - loss: 0.2844 - acc: 0.9139\n",
      "207/207 [==============================] - 71s 342ms/step - loss: 0.0716 - acc: 0.9833 - val_loss: 0.2844 - val_acc: 0.9139\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 7s 292ms/step - loss: 0.0463 - acc: 0.9822\n",
      "207/207 [==============================] - 70s 336ms/step - loss: 0.0627 - acc: 0.9810 - val_loss: 0.0463 - val_acc: 0.9822\n",
      "Number of parameters: 24,768,385\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training inceptionV3\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 7s 289ms/step - loss: 0.6359 - acc: 0.8197\n",
      "207/207 [==============================] - 54s 262ms/step - loss: 0.2458 - acc: 0.8967 - val_loss: 0.6359 - val_acc: 0.8197\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0938 - acc: 0.9658\n",
      "207/207 [==============================] - 50s 239ms/step - loss: 0.1272 - acc: 0.9516 - val_loss: 0.0938 - val_acc: 0.9658\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.2307 - acc: 0.9235\n",
      "207/207 [==============================] - 50s 239ms/step - loss: 0.1232 - acc: 0.9533 - val_loss: 0.2307 - val_acc: 0.9235\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 5s 211ms/step - loss: 0.1091 - acc: 0.9536\n",
      "207/207 [==============================] - 50s 242ms/step - loss: 0.1058 - acc: 0.9589 - val_loss: 0.1091 - val_acc: 0.9536\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.1086 - acc: 0.9631\n",
      "207/207 [==============================] - 50s 242ms/step - loss: 0.1351 - acc: 0.9528 - val_loss: 0.1086 - val_acc: 0.9631\n",
      "Number of parameters: 22,983,457\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training xception\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 10s 421ms/step - loss: 0.2743 - acc: 0.8975\n",
      "207/207 [==============================] - 87s 422ms/step - loss: 0.1749 - acc: 0.9307 - val_loss: 0.2743 - val_acc: 0.8975\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 8s 355ms/step - loss: 0.1969 - acc: 0.9303\n",
      "207/207 [==============================] - 84s 404ms/step - loss: 0.0766 - acc: 0.9721 - val_loss: 0.1969 - val_acc: 0.9303\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 8s 355ms/step - loss: 0.4864 - acc: 0.8320\n",
      "207/207 [==============================] - 84s 405ms/step - loss: 0.1071 - acc: 0.9654 - val_loss: 0.4864 - val_acc: 0.8320\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 8s 354ms/step - loss: 0.5466 - acc: 0.7760\n",
      "207/207 [==============================] - 84s 405ms/step - loss: 0.1296 - acc: 0.9633 - val_loss: 0.5466 - val_acc: 0.7760\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 8s 354ms/step - loss: 0.3692 - acc: 0.8579\n",
      "207/207 [==============================] - 84s 405ms/step - loss: 0.0934 - acc: 0.9704 - val_loss: 0.3692 - val_acc: 0.8579\n",
      "Number of parameters: 22,042,153\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training nasnet_large\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 64s 3s/step - loss: 0.2022 - acc: 0.9221\n",
      "207/207 [==============================] - 611s 3s/step - loss: 0.1642 - acc: 0.9416 - val_loss: 0.2022 - val_acc: 0.9221\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.1884 - acc: 0.9385\n",
      "207/207 [==============================] - 602s 3s/step - loss: 0.0676 - acc: 0.9742 - val_loss: 0.1884 - val_acc: 0.9385\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.5520 - acc: 0.8839\n",
      "207/207 [==============================] - 602s 3s/step - loss: 0.0445 - acc: 0.9838 - val_loss: 0.5520 - val_acc: 0.8839\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.4062 - acc: 0.9139\n",
      "207/207 [==============================] - 603s 3s/step - loss: 0.0464 - acc: 0.9830 - val_loss: 0.4062 - val_acc: 0.9139\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.7613 - acc: 0.8825\n",
      "207/207 [==============================] - 602s 3s/step - loss: 0.0423 - acc: 0.9836 - val_loss: 0.7613 - val_acc: 0.8825\n",
      "Number of parameters: 87,113,299\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training nasnet_mobile\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 7s 298ms/step - loss: 0.1081 - acc: 0.9481\n",
      "207/207 [==============================] - 58s 282ms/step - loss: 0.1529 - acc: 0.9381 - val_loss: 0.1081 - val_acc: 0.9481\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 5s 200ms/step - loss: 0.4710 - acc: 0.8579\n",
      "207/207 [==============================] - 54s 259ms/step - loss: 0.0949 - acc: 0.9628 - val_loss: 0.4710 - val_acc: 0.8579\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.0476 - acc: 0.9850\n",
      "207/207 [==============================] - 54s 259ms/step - loss: 0.1011 - acc: 0.9639 - val_loss: 0.0476 - val_acc: 0.9850\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 5s 202ms/step - loss: 0.0598 - acc: 0.9713\n",
      "207/207 [==============================] - 54s 261ms/step - loss: 0.0756 - acc: 0.9704 - val_loss: 0.0598 - val_acc: 0.9713\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.0655 - acc: 0.9727\n",
      "207/207 [==============================] - 54s 260ms/step - loss: 0.1126 - acc: 0.9637 - val_loss: 0.0655 - val_acc: 0.9727\n",
      "Number of parameters: 4,942,485\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training densenet121\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 8s 367ms/step - loss: 0.3369 - acc: 0.8511\n",
      "207/207 [==============================] - 73s 355ms/step - loss: 0.1676 - acc: 0.9314 - val_loss: 0.3369 - val_acc: 0.8511\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 6s 268ms/step - loss: 0.2725 - acc: 0.8675\n",
      "207/207 [==============================] - 68s 329ms/step - loss: 0.1012 - acc: 0.9653 - val_loss: 0.2725 - val_acc: 0.8675\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 6s 270ms/step - loss: 0.1706 - acc: 0.9290\n",
      "207/207 [==============================] - 68s 328ms/step - loss: 0.0898 - acc: 0.9671 - val_loss: 0.1706 - val_acc: 0.9290\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 6s 268ms/step - loss: 0.1568 - acc: 0.9303\n",
      "207/207 [==============================] - 68s 329ms/step - loss: 0.0514 - acc: 0.9827 - val_loss: 0.1568 - val_acc: 0.9303\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 6s 270ms/step - loss: 0.6391 - acc: 0.7623\n",
      "207/207 [==============================] - 68s 329ms/step - loss: 0.0764 - acc: 0.9735 - val_loss: 0.6391 - val_acc: 0.7623\n",
      "Number of parameters: 7,693,889\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training densenet169\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 10s 423ms/step - loss: 0.1489 - acc: 0.9440\n",
      "207/207 [==============================] - 88s 423ms/step - loss: 0.1380 - acc: 0.9460 - val_loss: 0.1489 - val_acc: 0.9440\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 8s 329ms/step - loss: 0.2774 - acc: 0.8989\n",
      "207/207 [==============================] - 83s 399ms/step - loss: 0.0669 - acc: 0.9750 - val_loss: 0.2774 - val_acc: 0.8989\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 8s 327ms/step - loss: 0.1550 - acc: 0.9467\n",
      "207/207 [==============================] - 83s 400ms/step - loss: 0.0861 - acc: 0.9701 - val_loss: 0.1550 - val_acc: 0.9467\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 8s 329ms/step - loss: 0.2953 - acc: 0.9167\n",
      "207/207 [==============================] - 83s 400ms/step - loss: 0.0417 - acc: 0.9832 - val_loss: 0.2953 - val_acc: 0.9167\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 0.2458 - acc: 0.9098\n",
      "207/207 [==============================] - 83s 400ms/step - loss: 0.0916 - acc: 0.9712 - val_loss: 0.2458 - val_acc: 0.9098\n",
      "Number of parameters: 13,626,945\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training densenet201\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 12s 526ms/step - loss: 0.0978 - acc: 0.9536\n",
      "207/207 [==============================] - 109s 526ms/step - loss: 0.1571 - acc: 0.9316 - val_loss: 0.0978 - val_acc: 0.9536\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 9s 410ms/step - loss: 0.0554 - acc: 0.9809\n",
      "207/207 [==============================] - 103s 497ms/step - loss: 0.0714 - acc: 0.9750 - val_loss: 0.0554 - val_acc: 0.9809\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 10s 415ms/step - loss: 0.0724 - acc: 0.9686\n",
      "207/207 [==============================] - 103s 498ms/step - loss: 0.0544 - acc: 0.9794 - val_loss: 0.0724 - val_acc: 0.9686\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 9s 409ms/step - loss: 0.0669 - acc: 0.9699\n",
      "207/207 [==============================] - 103s 496ms/step - loss: 0.0856 - acc: 0.9681 - val_loss: 0.0669 - val_acc: 0.9699\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 9s 412ms/step - loss: 0.1116 - acc: 0.9481\n",
      "207/207 [==============================] - 103s 499ms/step - loss: 0.0731 - acc: 0.9724 - val_loss: 0.1116 - val_acc: 0.9481\n",
      "Number of parameters: 19,437,121\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training inception_resnet_v2\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 23s 1s/step - loss: 0.0851 - acc: 0.9699\n",
      "207/207 [==============================] - 205s 992ms/step - loss: 0.2126 - acc: 0.9135 - val_loss: 0.0851 - val_acc: 0.9699\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 19s 832ms/step - loss: 0.0982 - acc: 0.9672\n",
      "207/207 [==============================] - 195s 944ms/step - loss: 0.1411 - acc: 0.9451 - val_loss: 0.0982 - val_acc: 0.9672\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 19s 834ms/step - loss: 0.1074 - acc: 0.9686\n",
      "207/207 [==============================] - 196s 946ms/step - loss: 0.1020 - acc: 0.9618 - val_loss: 0.1074 - val_acc: 0.9686\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 19s 831ms/step - loss: 0.0663 - acc: 0.9795\n",
      "207/207 [==============================] - 196s 945ms/step - loss: 0.0985 - acc: 0.9645 - val_loss: 0.0663 - val_acc: 0.9795\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 19s 830ms/step - loss: 0.1348 - acc: 0.9549\n",
      "207/207 [==============================] - 195s 944ms/step - loss: 0.1698 - acc: 0.9410 - val_loss: 0.1348 - val_acc: 0.9549\n",
      "Number of parameters: 55,255,265\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training vgg16\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 11s 495ms/step - loss: 0.0197 - acc: 0.9945\n",
      "207/207 [==============================] - 94s 453ms/step - loss: 0.0930 - acc: 0.9665 - val_loss: 0.0197 - val_acc: 0.9945\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.0159 - acc: 0.9904\n",
      "207/207 [==============================] - 87s 421ms/step - loss: 0.0215 - acc: 0.9914 - val_loss: 0.0159 - val_acc: 0.9904\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.0232 - acc: 0.9918\n",
      "207/207 [==============================] - 87s 421ms/step - loss: 0.0153 - acc: 0.9945 - val_loss: 0.0232 - val_acc: 0.9918\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.0820 - acc: 0.9795\n",
      "207/207 [==============================] - 87s 422ms/step - loss: 0.0098 - acc: 0.9961 - val_loss: 0.0820 - val_acc: 0.9795\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 9s 377ms/step - loss: 0.0180 - acc: 0.9932\n",
      "207/207 [==============================] - 87s 422ms/step - loss: 0.0081 - acc: 0.9968 - val_loss: 0.0180 - val_acc: 0.9932\n",
      "Number of parameters: 15,108,929\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training vgg19\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 10s 445ms/step - loss: 0.0286 - acc: 0.9904\n",
      "207/207 [==============================] - 102s 495ms/step - loss: 0.0719 - acc: 0.9735 - val_loss: 0.0286 - val_acc: 0.9904\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 10s 442ms/step - loss: 0.0485 - acc: 0.9877\n",
      "207/207 [==============================] - 102s 492ms/step - loss: 0.0281 - acc: 0.9904 - val_loss: 0.0485 - val_acc: 0.9877\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 10s 441ms/step - loss: 0.0271 - acc: 0.9904\n",
      "207/207 [==============================] - 102s 493ms/step - loss: 0.0133 - acc: 0.9951 - val_loss: 0.0271 - val_acc: 0.9904\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 10s 442ms/step - loss: 0.0343 - acc: 0.9904\n",
      "207/207 [==============================] - 102s 493ms/step - loss: 0.0121 - acc: 0.9956 - val_loss: 0.0343 - val_acc: 0.9904\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 10s 442ms/step - loss: 0.0451 - acc: 0.9918\n",
      "207/207 [==============================] - 102s 493ms/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0451 - val_acc: 0.9918\n",
      "Number of parameters: 20,418,625\n",
      "---------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "train_and_evaluate_models(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Hemos entrenado a cada candidato durante 5 _epochs_ con el fin de evaluar el poder de aprendizaje de cada modelo. Como establecimos con anterioridad, queremos mantener aquellos modelos que manifiesten un buen desempeño y que, a su vez, minimicen la complejidad (es decir, el número de parámetros).\n",
    "\n",
    "Es menester destacar que los modelos más grandes (tales como InceptionResnetV2, NASNetLarge y Xception), después de 5 _epochs_, presentan peor rendimiento que modelos cuya arquitectura es más simple, como MobileNet o NASNetMobile. Esto es entendible, puesto que mientras más grande es un modelo, más tiempo le toma aprender debido al alto volumen de parámetros.\n",
    "\n",
    "Debido a que nuestra data es relativamente pequeña y no tan compleja, pareciera que los modelos más pequeños lo hacen mejor.\n",
    "\n",
    "El podio de candidatos que promoveremos a la fase de optimización son:\n",
    "\n",
    " - **VGG16**. Número de parámetros: 15,108,929. Exactitud en el conjunto de validación: 99.32%.\n",
    " - **MobileNet**. Número de parámetros: 3,885,249.Exactitud en el conjunto de validación: 98.22%.\n",
    " - **NASNetMobile**. Número de parámetros: 4,942,485. Exactitud en el conjunto de validación: 97.27%.\n",
    " \n",
    "Aunque hay algunos modelos que presentan una exactitud un poco más alta que **MobileNet** y **NASNetMobile**, son mucho más grandes, por lo que el incremento en complejidad no vale la pena, dada la pequeña mejora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
