{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation\n",
    "\n",
    "We are now familiar with our data, so the next logical step is to explore the space of algorithms that will eventually yield a good model for the task we are trying to solve.\n",
    "\n",
    "Our goal in this notebook is not to develop state of the art solutions, but rather spotcheck several architectures in order to see which will be promoted to the next phase of the process, centered around model optimization. \n",
    "\n",
    "Without further ado, let's get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "In computer vision is always a good idea to start leveraging the knowledge of pre-trained models. This is known as transfer learning. \n",
    "\n",
    "Keras comes with a nice set of models trained on ImageNet, which is great. This time we'll use the Keras interface that comes bundled with TensorFlow, instead of the standalone version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spotchecking\n",
    "\n",
    "In order to evaluate a broad range of possible algorithms, we need to implement some methods first. Let's start by creating a function to load our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Given that we are spotchecking deep neural networks, we need to preserve as much memory as possible. For that reason, we are going to generate batches on demand directly from disk, using `flow_from_directory`. \n",
    "\n",
    "This function expects all the images corresponding to a class to be stored inside a subfolder with the name of that class. For that reason, we must reaccomodate our directory structure.\n",
    "\n",
    "We'll also set aside 10% of our data for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "destination_directory = './dataset'\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return image\n",
    "\n",
    "VALIDATION_PROPORTION = 0.1\n",
    "vehicles_images_path = shuffle(glob.glob('data/vehicles/*/*.png'))\n",
    "split_point = int(len(vehicles_images_path) * VALIDATION_PROPORTION)\n",
    "\n",
    "for i, image_path in enumerate(vehicles_images_path):\n",
    "    image = load_image(image_path)\n",
    "    \n",
    "    if i < split_point:\n",
    "        destination_path = os.path.join(destination_directory, 'valid', 'vehicle', f'{i}.png')\n",
    "    else:\n",
    "        destination_path = os.path.join(destination_directory, 'train', 'vehicle', f'{i}.png')\n",
    "    \n",
    "    cv2.imwrite(destination_path, image)\n",
    "    \n",
    "non_vehicles_images_path = shuffle(glob.glob('data/non-vehicles/*/*.png'))\n",
    "split_point = int(len(non_vehicles_images_path) * VALIDATION_PROPORTION)\n",
    "for i, image_path in enumerate(non_vehicles_images_path):\n",
    "    image = load_image(image_path)\n",
    "    \n",
    "    if i < split_point:\n",
    "        destination_path = os.path.join(destination_directory, 'valid', 'non_vehicle', f'{i}.png')\n",
    "    else:\n",
    "        destination_path = os.path.join(destination_directory, 'train', 'non_vehicle', f'{i}.png')\n",
    "    \n",
    "    cv2.imwrite(destination_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the examples and their labels in the `X` and `y` variables, respectively. We decided to use the following mapping for the labels:\n",
    "\n",
    "```\n",
    "1 --> Vehicle.\n",
    "0 --> Non-Vehicle.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "Now, lets's procede to define a method to retrieve all the models we want to evaluate. \n",
    "\n",
    "The `get_models` method will return a `dict` of triplets, where the keys correspond to the name of the pretrained model we are using, and the values are triplets where the first element contain the input preprocessing function corresponding to that pre-trained model, the second element is a function to construct the model itself and the last one contains the input size for that model.\n",
    "\n",
    "The `get_model_with_new_top` sub-function takes a base (pre-trained) model and attaches a fully connected network on top of it. It also freezes all the layers in the pre-trained model. Finally, it compiles the model to use `adam` as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "\n",
    "SEED = 314159\n",
    "\n",
    "def get_models(models=None):\n",
    "    # Takes a base, pretrained model, and attaches a new FCN on top of it.\n",
    "    def get_model_with_new_top(base_model):\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        predictions = Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "        model = Model(inputs=base_model.input, outputs=predictions)\n",
    "        \n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    if models is None:\n",
    "        models = dict()\n",
    "        \n",
    "    models['mobilenet'] = {\n",
    "        'preprocessing_function': applications.mobilenet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['resnet50'] = {\n",
    "        'preprocessing_function': applications.resnet50.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['inceptionV3'] = {\n",
    "        'preprocessing_function': applications.inception_v3.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['xception'] = {\n",
    "        'preprocessing_function': applications.xception.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.Xception(weights='imagenet', include_top=False, input_shape=(224, 224, 3))), \n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['nasnet_large'] = {\n",
    "        'preprocessing_function': applications.nasnet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.NASNetLarge(weights='imagenet', include_top=False, input_shape=(331, 331, 3))),\n",
    "        'input_shape': (331, 331)\n",
    "    }\n",
    "    \n",
    "    models['nasnet_mobile'] = {\n",
    "        'preprocessing_function': applications.nasnet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['densenet121'] = {\n",
    "        'preprocessing_function': applications.densenet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['densenet169'] = {\n",
    "        'preprocessing_function': applications.densenet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.DenseNet169(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['densenet201'] = {\n",
    "        'preprocessing_function': applications.densenet.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['inception_resnet_v2'] = {\n",
    "        'preprocessing_function': applications.inception_resnet_v2.preprocess_input,\n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(299, 299, 3))),\n",
    "        'input_shape': (299, 299)\n",
    "    }\n",
    "    \n",
    "    models['vgg16'] = {\n",
    "        'preprocessing_function': applications.vgg16.preprocess_input, \n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))), \n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    models['vgg19'] = {\n",
    "        'preprocessing_function': applications.vgg19.preprocess_input, \n",
    "        'model_constructor': lambda: get_model_with_new_top(applications.VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))), \n",
    "        'input_shape': (224, 224)\n",
    "    }\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(models, epochs=5):\n",
    "    for model_name, model_data in models.items():\n",
    "        m = model_data['model_constructor']()\n",
    "        train_data_generator = ImageDataGenerator(preprocessing_function=model_data['preprocessing_function']).flow_from_directory('./dataset/train', \n",
    "                                                                                                                                   target_size=model_data['input_shape'],\n",
    "                                                                                                                                   batch_size=32,\n",
    "                                                                                                                                   class_mode='binary')\n",
    "        \n",
    "        valid_data_generator = ImageDataGenerator(preprocessing_function=model_data['preprocessing_function']).flow_from_directory('./dataset/valid', \n",
    "                                                                                                                                   target_size=model_data['input_shape'],\n",
    "                                                                                                                                   batch_size=32,\n",
    "                                                                                                                                   class_mode='binary')\n",
    "        step_size_train = train_data_generator.n // train_data_generator.batch_size\n",
    "\n",
    "        print(f'Training {model_name}')\n",
    "        history = m.fit_generator(generator=train_data_generator,\n",
    "                                  steps_per_epoch=step_size_train,\n",
    "                                  validation_data=valid_data_generator,\n",
    "                                  validation_steps=(valid_data_generator.n // valid_data_generator.batch_size),\n",
    "                                  epochs=epochs)\n",
    "\n",
    "        print('Number of parameters: {:,}'.format(m.count_params()))\n",
    "        print('---------------\\n\\n')\n",
    "\n",
    "        del m\n",
    "        del history\n",
    "        K.clear_session()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training mobilenet\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 3s 136ms/step - loss: 0.1482 - acc: 0.9467\n",
      "207/207 [==============================] - 27s 132ms/step - loss: 0.1239 - acc: 0.9540 - val_loss: 0.1482 - val_acc: 0.9467\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 2s 102ms/step - loss: 0.0569 - acc: 0.9822\n",
      "207/207 [==============================] - 25s 120ms/step - loss: 0.0554 - acc: 0.9791 - val_loss: 0.0569 - val_acc: 0.9822\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 2s 102ms/step - loss: 0.1439 - acc: 0.9399\n",
      "207/207 [==============================] - 24s 118ms/step - loss: 0.0510 - acc: 0.9800 - val_loss: 0.1439 - val_acc: 0.9399\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 2s 102ms/step - loss: 0.0527 - acc: 0.9809\n",
      "207/207 [==============================] - 24s 117ms/step - loss: 0.0396 - acc: 0.9848 - val_loss: 0.0527 - val_acc: 0.9809\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 2s 102ms/step - loss: 0.0580 - acc: 0.9822\n",
      "207/207 [==============================] - 24s 117ms/step - loss: 0.0564 - acc: 0.9801 - val_loss: 0.0580 - val_acc: 0.9822\n",
      "Number of parameters: 3,885,249\n",
      "---------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training resnet50\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 8s 357ms/step - loss: 0.0534 - acc: 0.9809\n",
      "207/207 [==============================] - 74s 356ms/step - loss: 0.0916 - acc: 0.9630 - val_loss: 0.0534 - val_acc: 0.9809\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 7s 291ms/step - loss: 0.0938 - acc: 0.9672\n",
      "207/207 [==============================] - 70s 338ms/step - loss: 0.0523 - acc: 0.9809 - val_loss: 0.0938 - val_acc: 0.9672\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 7s 288ms/step - loss: 0.0544 - acc: 0.9768\n",
      "207/207 [==============================] - 70s 337ms/step - loss: 0.0279 - acc: 0.9909 - val_loss: 0.0544 - val_acc: 0.9768\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 7s 294ms/step - loss: 0.2844 - acc: 0.9139\n",
      "207/207 [==============================] - 71s 342ms/step - loss: 0.0716 - acc: 0.9833 - val_loss: 0.2844 - val_acc: 0.9139\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 7s 292ms/step - loss: 0.0463 - acc: 0.9822\n",
      "207/207 [==============================] - 70s 336ms/step - loss: 0.0627 - acc: 0.9810 - val_loss: 0.0463 - val_acc: 0.9822\n",
      "Number of parameters: 24,768,385\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training inceptionV3\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 7s 289ms/step - loss: 0.6359 - acc: 0.8197\n",
      "207/207 [==============================] - 54s 262ms/step - loss: 0.2458 - acc: 0.8967 - val_loss: 0.6359 - val_acc: 0.8197\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.0938 - acc: 0.9658\n",
      "207/207 [==============================] - 50s 239ms/step - loss: 0.1272 - acc: 0.9516 - val_loss: 0.0938 - val_acc: 0.9658\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.2307 - acc: 0.9235\n",
      "207/207 [==============================] - 50s 239ms/step - loss: 0.1232 - acc: 0.9533 - val_loss: 0.2307 - val_acc: 0.9235\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 5s 211ms/step - loss: 0.1091 - acc: 0.9536\n",
      "207/207 [==============================] - 50s 242ms/step - loss: 0.1058 - acc: 0.9589 - val_loss: 0.1091 - val_acc: 0.9536\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 5s 207ms/step - loss: 0.1086 - acc: 0.9631\n",
      "207/207 [==============================] - 50s 242ms/step - loss: 0.1351 - acc: 0.9528 - val_loss: 0.1086 - val_acc: 0.9631\n",
      "Number of parameters: 22,983,457\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training xception\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 10s 421ms/step - loss: 0.2743 - acc: 0.8975\n",
      "207/207 [==============================] - 87s 422ms/step - loss: 0.1749 - acc: 0.9307 - val_loss: 0.2743 - val_acc: 0.8975\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 8s 355ms/step - loss: 0.1969 - acc: 0.9303\n",
      "207/207 [==============================] - 84s 404ms/step - loss: 0.0766 - acc: 0.9721 - val_loss: 0.1969 - val_acc: 0.9303\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 8s 355ms/step - loss: 0.4864 - acc: 0.8320\n",
      "207/207 [==============================] - 84s 405ms/step - loss: 0.1071 - acc: 0.9654 - val_loss: 0.4864 - val_acc: 0.8320\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 8s 354ms/step - loss: 0.5466 - acc: 0.7760\n",
      "207/207 [==============================] - 84s 405ms/step - loss: 0.1296 - acc: 0.9633 - val_loss: 0.5466 - val_acc: 0.7760\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 8s 354ms/step - loss: 0.3692 - acc: 0.8579\n",
      "207/207 [==============================] - 84s 405ms/step - loss: 0.0934 - acc: 0.9704 - val_loss: 0.3692 - val_acc: 0.8579\n",
      "Number of parameters: 22,042,153\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training nasnet_large\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 64s 3s/step - loss: 0.2022 - acc: 0.9221\n",
      "207/207 [==============================] - 611s 3s/step - loss: 0.1642 - acc: 0.9416 - val_loss: 0.2022 - val_acc: 0.9221\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.1884 - acc: 0.9385\n",
      "207/207 [==============================] - 602s 3s/step - loss: 0.0676 - acc: 0.9742 - val_loss: 0.1884 - val_acc: 0.9385\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.5520 - acc: 0.8839\n",
      "207/207 [==============================] - 602s 3s/step - loss: 0.0445 - acc: 0.9838 - val_loss: 0.5520 - val_acc: 0.8839\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.4062 - acc: 0.9139\n",
      "207/207 [==============================] - 603s 3s/step - loss: 0.0464 - acc: 0.9830 - val_loss: 0.4062 - val_acc: 0.9139\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.7613 - acc: 0.8825\n",
      "207/207 [==============================] - 602s 3s/step - loss: 0.0423 - acc: 0.9836 - val_loss: 0.7613 - val_acc: 0.8825\n",
      "Number of parameters: 87,113,299\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training nasnet_mobile\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 7s 298ms/step - loss: 0.1081 - acc: 0.9481\n",
      "207/207 [==============================] - 58s 282ms/step - loss: 0.1529 - acc: 0.9381 - val_loss: 0.1081 - val_acc: 0.9481\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 5s 200ms/step - loss: 0.4710 - acc: 0.8579\n",
      "207/207 [==============================] - 54s 259ms/step - loss: 0.0949 - acc: 0.9628 - val_loss: 0.4710 - val_acc: 0.8579\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.0476 - acc: 0.9850\n",
      "207/207 [==============================] - 54s 259ms/step - loss: 0.1011 - acc: 0.9639 - val_loss: 0.0476 - val_acc: 0.9850\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 5s 202ms/step - loss: 0.0598 - acc: 0.9713\n",
      "207/207 [==============================] - 54s 261ms/step - loss: 0.0756 - acc: 0.9704 - val_loss: 0.0598 - val_acc: 0.9713\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 5s 199ms/step - loss: 0.0655 - acc: 0.9727\n",
      "207/207 [==============================] - 54s 260ms/step - loss: 0.1126 - acc: 0.9637 - val_loss: 0.0655 - val_acc: 0.9727\n",
      "Number of parameters: 4,942,485\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training densenet121\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 8s 367ms/step - loss: 0.3369 - acc: 0.8511\n",
      "207/207 [==============================] - 73s 355ms/step - loss: 0.1676 - acc: 0.9314 - val_loss: 0.3369 - val_acc: 0.8511\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 6s 268ms/step - loss: 0.2725 - acc: 0.8675\n",
      "207/207 [==============================] - 68s 329ms/step - loss: 0.1012 - acc: 0.9653 - val_loss: 0.2725 - val_acc: 0.8675\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 6s 270ms/step - loss: 0.1706 - acc: 0.9290\n",
      "207/207 [==============================] - 68s 328ms/step - loss: 0.0898 - acc: 0.9671 - val_loss: 0.1706 - val_acc: 0.9290\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 6s 268ms/step - loss: 0.1568 - acc: 0.9303\n",
      "207/207 [==============================] - 68s 329ms/step - loss: 0.0514 - acc: 0.9827 - val_loss: 0.1568 - val_acc: 0.9303\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 6s 270ms/step - loss: 0.6391 - acc: 0.7623\n",
      "207/207 [==============================] - 68s 329ms/step - loss: 0.0764 - acc: 0.9735 - val_loss: 0.6391 - val_acc: 0.7623\n",
      "Number of parameters: 7,693,889\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training densenet169\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 10s 423ms/step - loss: 0.1489 - acc: 0.9440\n",
      "207/207 [==============================] - 88s 423ms/step - loss: 0.1380 - acc: 0.9460 - val_loss: 0.1489 - val_acc: 0.9440\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 8s 329ms/step - loss: 0.2774 - acc: 0.8989\n",
      "207/207 [==============================] - 83s 399ms/step - loss: 0.0669 - acc: 0.9750 - val_loss: 0.2774 - val_acc: 0.8989\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 8s 327ms/step - loss: 0.1550 - acc: 0.9467\n",
      "207/207 [==============================] - 83s 400ms/step - loss: 0.0861 - acc: 0.9701 - val_loss: 0.1550 - val_acc: 0.9467\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 8s 329ms/step - loss: 0.2953 - acc: 0.9167\n",
      "207/207 [==============================] - 83s 400ms/step - loss: 0.0417 - acc: 0.9832 - val_loss: 0.2953 - val_acc: 0.9167\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 8s 328ms/step - loss: 0.2458 - acc: 0.9098\n",
      "207/207 [==============================] - 83s 400ms/step - loss: 0.0916 - acc: 0.9712 - val_loss: 0.2458 - val_acc: 0.9098\n",
      "Number of parameters: 13,626,945\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training densenet201\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 12s 526ms/step - loss: 0.0978 - acc: 0.9536\n",
      "207/207 [==============================] - 109s 526ms/step - loss: 0.1571 - acc: 0.9316 - val_loss: 0.0978 - val_acc: 0.9536\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 9s 410ms/step - loss: 0.0554 - acc: 0.9809\n",
      "207/207 [==============================] - 103s 497ms/step - loss: 0.0714 - acc: 0.9750 - val_loss: 0.0554 - val_acc: 0.9809\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 10s 415ms/step - loss: 0.0724 - acc: 0.9686\n",
      "207/207 [==============================] - 103s 498ms/step - loss: 0.0544 - acc: 0.9794 - val_loss: 0.0724 - val_acc: 0.9686\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 9s 409ms/step - loss: 0.0669 - acc: 0.9699\n",
      "207/207 [==============================] - 103s 496ms/step - loss: 0.0856 - acc: 0.9681 - val_loss: 0.0669 - val_acc: 0.9699\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 9s 412ms/step - loss: 0.1116 - acc: 0.9481\n",
      "207/207 [==============================] - 103s 499ms/step - loss: 0.0731 - acc: 0.9724 - val_loss: 0.1116 - val_acc: 0.9481\n",
      "Number of parameters: 19,437,121\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training inception_resnet_v2\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 23s 1s/step - loss: 0.0851 - acc: 0.9699\n",
      "207/207 [==============================] - 205s 992ms/step - loss: 0.2126 - acc: 0.9135 - val_loss: 0.0851 - val_acc: 0.9699\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 19s 832ms/step - loss: 0.0982 - acc: 0.9672\n",
      "207/207 [==============================] - 195s 944ms/step - loss: 0.1411 - acc: 0.9451 - val_loss: 0.0982 - val_acc: 0.9672\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 19s 834ms/step - loss: 0.1074 - acc: 0.9686\n",
      "207/207 [==============================] - 196s 946ms/step - loss: 0.1020 - acc: 0.9618 - val_loss: 0.1074 - val_acc: 0.9686\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 19s 831ms/step - loss: 0.0663 - acc: 0.9795\n",
      "207/207 [==============================] - 196s 945ms/step - loss: 0.0985 - acc: 0.9645 - val_loss: 0.0663 - val_acc: 0.9795\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 19s 830ms/step - loss: 0.1348 - acc: 0.9549\n",
      "207/207 [==============================] - 195s 944ms/step - loss: 0.1698 - acc: 0.9410 - val_loss: 0.1348 - val_acc: 0.9549\n",
      "Number of parameters: 55,255,265\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training vgg16\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 11s 495ms/step - loss: 0.0197 - acc: 0.9945\n",
      "207/207 [==============================] - 94s 453ms/step - loss: 0.0930 - acc: 0.9665 - val_loss: 0.0197 - val_acc: 0.9945\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.0159 - acc: 0.9904\n",
      "207/207 [==============================] - 87s 421ms/step - loss: 0.0215 - acc: 0.9914 - val_loss: 0.0159 - val_acc: 0.9904\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.0232 - acc: 0.9918\n",
      "207/207 [==============================] - 87s 421ms/step - loss: 0.0153 - acc: 0.9945 - val_loss: 0.0232 - val_acc: 0.9918\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 9s 378ms/step - loss: 0.0820 - acc: 0.9795\n",
      "207/207 [==============================] - 87s 422ms/step - loss: 0.0098 - acc: 0.9961 - val_loss: 0.0820 - val_acc: 0.9795\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 9s 377ms/step - loss: 0.0180 - acc: 0.9932\n",
      "207/207 [==============================] - 87s 422ms/step - loss: 0.0081 - acc: 0.9968 - val_loss: 0.0180 - val_acc: 0.9932\n",
      "Number of parameters: 15,108,929\n",
      "---------------\n",
      "\n",
      "\n",
      "Found 6593 images belonging to 2 classes.\n",
      "Found 732 images belonging to 2 classes.\n",
      "Training vgg19\n",
      "Epoch 1/5\n",
      "23/23 [==============================] - 10s 445ms/step - loss: 0.0286 - acc: 0.9904\n",
      "207/207 [==============================] - 102s 495ms/step - loss: 0.0719 - acc: 0.9735 - val_loss: 0.0286 - val_acc: 0.9904\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 10s 442ms/step - loss: 0.0485 - acc: 0.9877\n",
      "207/207 [==============================] - 102s 492ms/step - loss: 0.0281 - acc: 0.9904 - val_loss: 0.0485 - val_acc: 0.9877\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 10s 441ms/step - loss: 0.0271 - acc: 0.9904\n",
      "207/207 [==============================] - 102s 493ms/step - loss: 0.0133 - acc: 0.9951 - val_loss: 0.0271 - val_acc: 0.9904\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 10s 442ms/step - loss: 0.0343 - acc: 0.9904\n",
      "207/207 [==============================] - 102s 493ms/step - loss: 0.0121 - acc: 0.9956 - val_loss: 0.0343 - val_acc: 0.9904\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 10s 442ms/step - loss: 0.0451 - acc: 0.9918\n",
      "207/207 [==============================] - 102s 493ms/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0451 - val_acc: 0.9918\n",
      "Number of parameters: 20,418,625\n",
      "---------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "train_and_evaluate_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
