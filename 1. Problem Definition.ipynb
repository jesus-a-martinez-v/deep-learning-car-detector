{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Definition\n",
    "\n",
    "_An image is worth a thousand words._\n",
    "\n",
    "How many times have you heard this? I know, I know... It is clich√©, but also true. \n",
    "\n",
    "This little bit of wisdom greatly justifies the existence of a whole field dedicated to understanding images, their meaning and what actions can be derived based on the information extracted from them. Of course, we are talking about computer vision.\n",
    "\n",
    "But what does \"understanding\" actually mean? Is it interpreting a scene? Telling what's in a photo? Describing the relation between components in an image? All of it, really. \n",
    "\n",
    "Of course, each of the angles to \"understanding\" mentioned above are related to different tasks within computer vision, being object detection one of the oldest.\n",
    "\n",
    "In this project we'll develop a model too discern between cars and non-cars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why car detection is important?\n",
    "\n",
    "<img src=\"assets/car_boxes.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, telling if an object appears in a given image is not relevant enough. It is like asking a database if a particular record exists. The real importance of the question lies in the _why_.\n",
    "\n",
    "_Why_ do you want to know? What are you going to do with this information?\n",
    "\n",
    "In other words, it's the application of the information, not the information itself that is important. \n",
    "\n",
    "Here are several scenarios where car detection might be useful:\n",
    "\n",
    " - **Inventory**: If you're a big car manufacturer, such as GM, Ford or Chrysler a piece of software capable of counting the number of cars in an image could greatly aid you with your inventory keeping process.\n",
    " - **Autonomous driving**: A self-driving car, in order to safely deliver its passengers or cargo from point A to point B must be deeply aware of its surroundings. A car detection algorithm could provide valuable information to the many sensors an autonomous vehicle has, in order to make important decisions, such as changing lanes or parking.\n",
    " - **Security**: In areas where the access is restricted, such as private complex, airports or military bases, a model trained to identify certain vehicles could act as a trespassing alarm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you identify a car?\n",
    "\n",
    "<img src=\"assets/red_beetle.jpeg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are an EXPERT!\n",
    "\n",
    "Yeah, you! I'm convinced that you could identify a car even if it's from a brand you've never heard of. \n",
    "\n",
    "Heck, you could probably detect a cartoon car, a three-wheeled car, an F1 car, and so on. How? Because we humans are built to spot objects in a scene. We might not be able to explain the nuts and bolts of what's happening in our brain... We just can.\n",
    "\n",
    "The neuroscientist in me would say that, probably, our brain focuses on a couple of distinguishing aspects of what makes a car a car: Wheels (most of the time, four of them), head and tail lights, hood, trunk, etc. The typical environment of cars also give a way valuable information: It is more likely that a four-wheeled object is a car if it is surrounded by buildings or is on top of a road instead of clouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would a machine identify a car?\n",
    "\n",
    "<img src=\"assets/yolo.jpg\" />\n",
    "\n",
    "Here's where it gets interesting.\n",
    "\n",
    "Let's first remember that the images we see are just a bunch of numbers to a machine. Where we see a beautiful Cadillac driving to the sunset, a computer sees a giant tensor of integers. \n",
    "\n",
    "So, a machine must be able of find patterns within this sea of numbers in order to tell if there's a car somewhere in there.\n",
    "\n",
    "There a couple of approaches:\n",
    "\n",
    " - **Template matching**: We can compare an image with a template of a car and see how different they are. Let's say we perform a pixel-wise subtraction between these two images in order to come up with a measure of similarity. If the number is low, it means that the images match in most of the regions. If the quantity is large, then they are not so similar. There are many problems with this approach:\n",
    "     - What happens if the car in the new image differ in color with the one in our template?.\n",
    "     - What if the car in the new image is on the right and in our template appears on the left?\n",
    "     - What if the car is the new image is very far away and the one in the template is very close to the camera?\n",
    "     - What is the new image contains a close-up of Clifford, the red dog and the template contains a red firetruck? Most of the image would be red, but I'm pretty sure Clifford is not a truck.\n",
    " - **Classic CV + Classic Machine Learning**: This approach is much more robust than mere template matching. Basically, it consists of extracting meaningful features from the image using classic computer vision techniques, such as Histogram of Oriented Gradients, in order to feed a classifier, like a decision tree, a support vector machine or a logistic regressor. I implemented a vehicle detector using this approach. You can access the project [here](https://github.com/jesus-a-martinez-v/vehicle-detection).\n",
    " - **Deep Learning**: This is the most powerful approach to any kind of object detection problem. Many groundbreaking neural network architectures have been developed to not only make better classifications, but also faster. One of the most interesting is YOLO. Here's a video of YOLO watching YouTube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/r6ZzopHEO1U\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb960101828>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('https://www.youtube.com/embed/r6ZzopHEO1U', width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we'll use the latter approach.\n",
    "\n",
    "In the next notebook we'll proceed to explore our data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
